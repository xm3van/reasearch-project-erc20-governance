Step1: Filter out unique_addresses out of transactions_merged_all and store in new df
awk -F, '(NR==FNR){a[$2];next}(($6 in a) || ($7 in a))' df_unique_addresses.csv transactions_merged_all.csv > tx_all_uniq_addresses.csv

Step2: Remove columns 
tx_all_uniq_addresses.csv 
cut -d, -f2,3,5,11,13-15 --complement tx_all_uniq_addresses_selected.csv 

Rational: 
- Irrelevant for culstering heuristic

Ref.: https://linuxconfig.org/how-to-remove-columns-from-csv-based-on-column-number-using-bash-shell

----





Out-takes 

awk -F, '{ if(($6 in unique_addresses) || ($7 in unique_addresses)) { print } }' transactions_merged_all.csv > test.csv

https://www.tim-dennis.com/data/tech/2016/08/09/using-awk-filter-rows.html
http://tuxgraphics.org/~guido/scripts/awk-one-liner.html


Requirements:
filter df_unique_addresses.csv out of 'transactions_merged_all.csv' if its in 
column 6 or 7 (double check if index correct)


awk -F, '(NR==FNR){a[$2];next}(($6 in a) || ($7 in a))' df_unique_addresses.csv transactions_merged_all.csv > tx_all_uniq_addresses.csv


awk -F, '$2 ~ /jpn|por/ {print}' transactions_merged_all.csv > tx_all_unique_addresses.csv

awk -F, '(NR==FNR){a[$2];next}!([$6 in a] || [$7 in a])' df_unique_addresses.csv transactions_merged_all.csv


Explanations:

awk for column-based operations
-F, to define the column separator/tokenizer, in this case I use a comma
$2 ~ /jpn|por/ tests column #2 with expression /jpn|por/
$2 is column #2
/jpn|por/ is a regular expression to match jpn or por
{print} specifies what the awk should output if it found a matching line
print to print the whole input line (alternatively, print $3 will just print column #3)
... file1.csv specifies to read from an input file instead of stdin

Ref.: https://superuser.com/questions/827539/grep-to-filter-gigantic-csv-file/827575




#### Print 
 "Zeroth: ${array[0]}" 
awk -F, '{print $3" "$4" ""{" block_number: $2, hash: $1, value: $5}' tx_all_uniq_addresses_reduced.csv - faulty
awk -F,'${$3} ${4} {block_number: $2, hash: $1, value: $5}' tx_all_uniq_addresses_reduced.csv - faulty


awk -F, '{print $3" "$4" " "{" "\"block_number\"" ": " $2", "\"hash"\": " $1", 'value': " $5", 'gas': " $6", 'gas_price': " $7", 'block_timestamp': " $8"}"}' tx_all_uniq_addresses_reduced.csv

awk -F, '{print $3" "$4" " "{" "\"block_number\"" ": " $2", "\"hash\"" ": " $1", "\"value\"": " $5", "\"gas\"": " $6", "\"gas_price\"": " $7", "\"block_timestamp\"": " $8"}"}' tx_all_uniq_addresses_reduced.csv

awk -F, '{print $3" "$4" ""{" "\"block_number\"" ": " $2", "\"hash\"" ": " $1"}"}' tx_all_uniq_addresses_reduced.csv




awk -F, '{print $3" "$4" " "{" "\"block_number\"" ": " $2", "\"hash\"" ": " $1", "\"value\"": " $5", "\"gas_price\"": " $6"}"}' tx_all_uniq_addresses2_reduced.csv 
awk -F, '{print $3" "$4" ""{" ""block_number"" ": " $2", ""hash"" ": " $1}' tx_all_uniq_addresses_reduced.csv



### Create edge list 

awk -F, '{print $3" "$4" " "{" "\"block_number\"" ": " $2 ", " "\"hash\"" ": " $1", " "\"value\"" ": " $5 ", " "\"gas_price\"" ": " $6"}"}' tx_all_uniq_addresses2_reduced.csv > tx_all_uniq_addresses.edgelist
awk -F, '{print $3" "$4}' tx_all_uniq_addresses2_reduced.csv > tx_all_uniq_addresses2_simple.edgelist


### replace adddress values 

## wrapped NXM for NXM 
sed 's/0x0d438f3b5175bebc262bf23753c1e53d03432bde/0x3432b6a60d23ca0dfca7761b7ab56459d9c964d0/g' tx_all_uniq_addresses2_simple.edgelist > tx_all_simple_modified.edgelist

## rocket pool old for rocket pool new 
sed -i 's/0xB4EFd85c19999D84251304bDA99E90B92300Bd93/0xd33526068d116ce69f19a9ee46f0bd304f21a51f/g' tx_all_simple_modified.edgelist