{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "path = os.environ['PROJECT_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of unique sender & receiver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETAINED METHODOLOGICAL DOCUMENTATION\n",
    "\n",
    "# load unique governance tokens transfers \n",
    "df_tt = pd.read_csv(join( path, 'token_transfers_22021209_refined.csv'))\n",
    "\n",
    "# unique sender\n",
    "unique_sender = df_tt.from_address.unique()\n",
    "\n",
    "# unique receivr \n",
    "unique_receiver = df_tt.to_address.unique()\n",
    "\n",
    "# sender & receiver  \n",
    "array_SR= np.concatenate([unique_receiver, unique_sender])\n",
    "\n",
    "# remove dublicates between unique sender & receiver \n",
    "unique_addresses = np.unique(array_SR)\n",
    "\n",
    "# convert to csv for storage \n",
    "df_unique_addresses = pd. DataFrame(unique_addresses, columns=['unique_addresses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove burner addresses \n",
    "known_burner_addresses = ['0x0000000000000000000000000000000000000000',\n",
    "                        '0x0000000000000000000000000000000000000000',\n",
    "                        '0x0000000000000000000000000000000000000001',\n",
    "                        '0x0000000000000000000000000000000000000002',\n",
    "                        '0x0000000000000000000000000000000000000003',\n",
    "                        '0x0000000000000000000000000000000000000004',\n",
    "                        '0x0000000000000000000000000000000000000005',\n",
    "                        '0x0000000000000000000000000000000000000006',\n",
    "                        '0x0000000000000000000000000000000000000007',\n",
    "                        '0x000000000000000000000000000000000000dead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_unique_addresses_filtered = df_unique_addresses[df_unique_addresses.unique_addresses.isin(known_burner_addresses) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe dataset as df as csv\n",
    "df_unique_addresses_filtered.to_csv(join(path, 'df_unique_addresses2.csv') )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_addresses_filtered = pd.read_csv(join(path, 'df_unique_addresses2.csv' ))\n",
    "df_unique_addresses_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bash Manipulation\n",
    "Step1: consolidate all csv files using ubuntu comand line: \n",
    "> nohup awk '(NR == 1) || (FNR > 1)' transactions_merged_to_11307940.csv transactions_to_15050010.csv > transactions_merged_all.csv\n",
    "\n",
    "\n",
    "Step2: Filter out unique_addresses out of transactions_merged_all and store in new df\n",
    "> awk -F, '(NR==FNR){a[$2];next}(($6 in a) || ($7 in a))' df_unique_addresses2.csv transactions_merged_all.csv > tx_all_uniq_addresses2.csv\n",
    "\n",
    "Step3: Remove columns \n",
    "> cut -d, -f2,3,5,11,13-15 --complement tx_all_uniq_addresses_selected.csv \n",
    "\n",
    "Rational: \n",
    "- Irrelevant for culstering heuristic\n",
    "\n",
    "Ref.: https://linuxconfig.org/how-to-remove-columns-from-csv-based-on-column-number-using-bash-shell"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding on relevant columns in Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "       'hash', # unique identifier of tx\n",
    "       'nonce', # --> remove\n",
    "       'block_hash', # --> remove\n",
    "       'block_number', # necessary when filtering for snapshot dates\n",
    "       'transaction_index', # --> remove\n",
    "       'from_address', # --> network node \n",
    "       'to_address', # --> network node\n",
    "       'value', # --> weight \n",
    "       'gas', # --> clustering heuristic gas pattern (Beres et al. 2020)\n",
    "       'gas_price', # --> clustering heuristic gas pattern (Beres et al. 2020)\n",
    "       'input', # --> indicates smart contract interaction\n",
    "       'block_timestamp', # --> remove can be inferred from blockheight\n",
    "       'max_fee_per_gas', # -->\n",
    "       'max_priority_fee_per_gas', # -->\n",
    "       'transaction_type'# -->\n",
    "       ]\n",
    "\n",
    "['hash', 'block_number','from_address', 'to_address', 'value', 'gas', 'gas_price', 'input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, col in enumerate(columns): \n",
    "    print(f'Col: {col} || Col Num: {ind+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_sample_tx = dd.read_csv(join(path,'raw/transactions_to_11286141.csv')) #number of rows 1,622,714,622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_sample_tx.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select relevant columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(join(path,'tx_all_uniq_addresses_reduced.csv'), dtype='str', names=['hash', 'block_number','from_address', 'to_address', 'value', 'gas', 'gas_price','block_timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data into Edge List Format\n",
    "\n",
    "- Reference: https://networkx.org/documentation/stable/reference/readwrite/edgelist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "G = nx.read_edgelist(join(path, file), data=False)\n",
    "file = 'tx_all_uniq_addresses.edgelist'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkit import Graph\n",
    "import networkit as nk\n",
    "\n",
    "g = Graph()\n",
    "reader = nk.graphio.LineFileReader(path=\"edge_simple_test.txt\")\n",
    "graph = Graph(reader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nk.graphio.EdgeListReader(\"edge_simple_test.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.graphio.Format.EdgeListSpaceOne('edge_simple_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkit import GraphReader\n",
    "\n",
    "gr = GraphReader(\"edge_simple_test.txt\")\n",
    "g = gr.readGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig \n",
    "\n",
    "g = ig.Graph.TupleList('edge_simple_test.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "G.add_edge(1, 2, weight=7, color=\"red\")\n",
    "# nx.write_edgelist(G, \"test1.edgelist\", data=[\"color\", \"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist(\"test.edgelist\", data=True)\n",
    "# list(G.edges(data=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other & Out-Takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### removing tokens \n",
    "import web3 as Web3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Web3.Web3.toChecksumAddress('0x3432b6a60d23ca0dfca7761b7ab56459d9c964d0')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grep command to delete Gnosis( ) and Frax-Share ( ), as Gnosis is fragmenting in terms of Governance power and Frax is a non-standard ERC20\n",
    "\n",
    "> grep -v -e '0x6810e776880C02933D47DB1b9fc05908e5386b96' -e '0x3432b6a60d23ca0dfca7761b7ab56459d9c964d0' token_transfers_22021209.csv > token_transfers_22021209_refined.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
