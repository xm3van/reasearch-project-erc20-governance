{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e2b8d65-9a6b-411d-b686-638e78562329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/eisermann/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/user/eisermann/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:59: UserWarning: Pandas requires version '1.3.2' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot for Block Height: 11659570\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnapshot for Block Height: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m## formating of data\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m ddf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_balance_lookup_tables_labelled/df_token_balenace_labelled_greater_01pct_bh\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msnapshot\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_v2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m## rename column\u001b[39;00m\n\u001b[1;32m    106\u001b[0m ddf \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_x\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "#### Limit usage \n",
    "# ulimit -v 314572800  # 60% of 512 GB in kilobytes\n",
    "# ulimit -t 78 ## CPU usage of a process to 60% = 76.8\n",
    "####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import hypergeom \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import csv\n",
    "from itertools import combinations, islice\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "\n",
    "#custom functions \n",
    "# from wallet_hypergeom import main_wallets\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "path = os.environ[\"PROJECT_PATH\"]\n",
    "\n",
    "\n",
    "# snapshot selection\n",
    "df_snapshot = pd.read_csv(\"../assets/snapshot_selection.csv\")\n",
    "\n",
    "# address selection\n",
    "df_addresses = pd.read_csv(\"../assets/df_final_token_selection_20221209.csv\")\n",
    "\n",
    "# burner addresses\n",
    "# remove burner addresses\n",
    "known_burner_addresses = [\n",
    "    \"0x0000000000000000000000000000000000000000\",\n",
    "    \"0x0000000000000000000000000000000000000000\",\n",
    "    \"0x0000000000000000000000000000000000000001\",\n",
    "    \"0x0000000000000000000000000000000000000002\",\n",
    "    \"0x0000000000000000000000000000000000000003\",\n",
    "    \"0x0000000000000000000000000000000000000004\",\n",
    "    \"0x0000000000000000000000000000000000000005\",\n",
    "    \"0x0000000000000000000000000000000000000006\",\n",
    "    \"0x0000000000000000000000000000000000000007\",\n",
    "    \"0x000000000000000000000000000000000000dead\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "output_path = '/local/scratch/exported/governance-erc20/project_erc20_governance_data/wallet_projection_output/output_f-none'\n",
    "\n",
    "def main_wallets(wallet1, wallet2, pop_size):\n",
    "\n",
    "    # unique token a given address holds\n",
    "    wallet1_uniqT = dff[dff.address == wallet1].token_address.unique() # poor practice but i think saves storage\n",
    "    wallet2_uniqT = dff[dff.address == wallet2].token_address.unique()\n",
    "\n",
    "    # calcualte intersection in token holdings (binary)\n",
    "    wallet1_wallet2_uniqT_intersection = np.intersect1d(wallet1_uniqT, wallet2_uniqT, assume_unique=True)\n",
    "\n",
    "\n",
    "    # calculate hyptogeometric\n",
    "    M = pop_size  # number of token at a given snapshot    \n",
    "    n = len(wallet1_uniqT)  # number of draws - number of token wallets1\n",
    "    K = len(wallet2_uniqT)  # number of successes in population - number of token wallets2\n",
    "    x = len(wallet1_wallet2_uniqT_intersection) # number of successes in draws - intersection of tokens in wallet1 and wallet2\n",
    "\n",
    "    # Compute the cumulative probability of obtaining at most x successes\n",
    "    pvalue = 1 - hypergeom.cdf(x, M, n, K)\n",
    "\n",
    "    return pvalue\n",
    "\n",
    "# Define the function that will process a batch of combinations\n",
    "def process_batch(batch, pop_size, ddf):\n",
    "    results = []\n",
    "    for combination in batch:\n",
    "        # Call your function here to process the combination\n",
    "        result = main_wallets(combination[0], combination[1], pop_size, ddf)\n",
    "        results.append((combination[0], combination[1], result))\n",
    "    return results\n",
    "\n",
    "\n",
    "### saves how much circulating supply on ethereum has been covered \n",
    "pct_supply_coverage = {} \n",
    "\n",
    "for snapshot in df_snapshot[df_snapshot['Block Height'] >= 11547458]['Block Height']:\n",
    "    \n",
    "    print(f\"Snapshot for Block Height: {snapshot}\") \n",
    "\n",
    "    ## formating of data\n",
    "    # load data\n",
    "    ddf = pd.read_csv(\n",
    "        join(\n",
    "            path,\n",
    "            f\"token_balance_lookup_tables_labelled/df_token_balenace_labelled_greater_01pct_bh{snapshot}_v2.csv\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ## rename column\n",
    "    ddf = ddf.rename(columns={'address_x': 'address'})\n",
    "\n",
    "    # filter data\n",
    "    ddf = ddf[ddf.value > 0]\n",
    "    ddf = ddf[ddf.token_address.isin(df_addresses.address) == True]\n",
    "\n",
    "    # remove known burner addresses\n",
    "    ddf = ddf[ddf.address.isin(known_burner_addresses) == False]\n",
    "    \n",
    "    # we set a cut of point of 0.001% \n",
    "    ddf = ddf[ddf.pct_supply > 0.00001]  \n",
    "    \n",
    "    # save coverage \n",
    "    pct_supply_coverage[str(snapshot)] = dict(ddf.groupby('token_address').pct_supply.sum())\n",
    "    \n",
    "   \n",
    "    # population is the size of the possible token that can be held in the sample \n",
    "    pop_size = len(ddf.token_address.unique())\n",
    "    \n",
    "    \n",
    "    # Define the batch size for computing the p-values\n",
    "    batch_size = 100\n",
    "    \n",
    "    print(f\"Data Frame Loaded\")\n",
    "\n",
    "\n",
    "    combinations_list = ((a, b) for a, b in combinations(ddf.address.unique(), 2))\n",
    "    \n",
    "    print(f\"Combination list finalised: combinations_list\")\n",
    "\n",
    "    # Divide the combinations into batches\n",
    "    batches = [combinations_list[i:i+batch_size] for i in range(0, len(combinations_list), batch_size)]\n",
    "\n",
    "    # Create a ProcessPoolExecutor with the desired number of processes\n",
    "    with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "        results = []\n",
    "\n",
    "        pbar = tqdm(total=len(combinations_list), desc=\"Processing combinations\", leave=False)\n",
    "\n",
    "        # Process each batch in parallel\n",
    "        for batch in batches:\n",
    "            batch_result = executor.submit(process_batch, batch, pop_size)\n",
    "            results.append(batch_result)\n",
    "\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "        # Wait for all results to be returned\n",
    "        with open(join(output_path, f'pvalues_wallets_{snapshot}.csv'), 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for future in tqdm(concurrent.futures.as_completed(results), total=len(results), desc=\"Saving results\", leave=False):\n",
    "                batch_result = future.result()\n",
    "                # Do something with the batch result, like save it to a file\n",
    "                for result in batch_result:\n",
    "                    # Do something with the individual result, like save it to a file\n",
    "                    writer.writerow(result)\n",
    "                    \n",
    "df = pd.from_dict(pct_supply_coverage)\n",
    "df.to_csv(join(output_path, \"pct_supply_coverage.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10debe21-e7b9-43ad-b68b-0db27769c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/eisermann/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/user/eisermann/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:59: UserWarning: Pandas requires version '1.3.2' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot for Block Height: 11659570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 40787it [00:09, 4355.69it/s]                           \n",
      "Collecting results:   1%|          | 207/40787 [00:21<1:09:40,  9.71it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import hypergeom \n",
    "\n",
    "from itertools import combinations, islice\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "#custom functions \n",
    "# from wallet_hypergeom import main_wallets\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "path = os.environ[\"PROJECT_PATH\"]\n",
    "\n",
    "\n",
    "# snapshot selection\n",
    "df_snapshot = pd.read_csv(\"../assets/snapshot_selection.csv\")\n",
    "\n",
    "# address selection\n",
    "df_addresses = pd.read_csv(\"../assets/df_final_token_selection_20221209.csv\")\n",
    "\n",
    "# burner addresses\n",
    "# remove burner addresses\n",
    "known_burner_addresses = [\n",
    "    \"0x0000000000000000000000000000000000000000\",\n",
    "    \"0x0000000000000000000000000000000000000000\",\n",
    "    \"0x0000000000000000000000000000000000000001\",\n",
    "    \"0x0000000000000000000000000000000000000002\",\n",
    "    \"0x0000000000000000000000000000000000000003\",\n",
    "    \"0x0000000000000000000000000000000000000004\",\n",
    "    \"0x0000000000000000000000000000000000000005\",\n",
    "    \"0x0000000000000000000000000000000000000006\",\n",
    "    \"0x0000000000000000000000000000000000000007\",\n",
    "    \"0x000000000000000000000000000000000000dead\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_path = '/local/scratch/exported/governance-erc20/project_erc20_governance_data/wallet_projection_output/output_f-none'\n",
    "\n",
    "def main_wallets(wallet1, wallet2, pop_size, ddf):\n",
    "\n",
    "    # unique token a given address holds\n",
    "    wallet1_uniqT = ddf[ddf.address == wallet1].token_address.unique()\n",
    "    wallet2_uniqT = ddf[ddf.address == wallet2].token_address.unique()\n",
    "\n",
    "    # calcualte intersection in token holdings (binary)\n",
    "    wallet1_wallet2_uniqT_intersection = np.intersect1d(wallet1_uniqT,wallet2_uniqT, assume_unique=True)\n",
    "\n",
    "    # calculate hyptogeometric\n",
    "    M = pop_size  # number of token at a given snapshot    \n",
    "    n = len(wallet1_uniqT)  # number of draws - number of token wallets1\n",
    "    K = len(wallet2_uniqT)  # number of successes in population - number of token wallets2\n",
    "    x = len(wallet1_wallet2_uniqT_intersection) # number of successes in draws - intersection of tokens in wallet1 and wallet2\n",
    "\n",
    "    # Compute the cumulative probability of obtaining at most x successes\n",
    "    pvalue = 1 - hypergeom.cdf(x, M, n, K)\n",
    "\n",
    "    return wallet1, wallet2, pvalue\n",
    "\n",
    "# Define the function that will process a batch of combinations\n",
    "def process_batch(batch):\n",
    "    results = []\n",
    "    for combination in batch:\n",
    "        # Call your function here to process the combination\n",
    "        result = main_wallets(combination[0], combination[1], pop_size, ddf)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "### saves how much circulating supply on ethereum has been covered \n",
    "pct_supply_coverage = {} \n",
    "\n",
    "for snapshot in df_snapshot[df_snapshot['Block Height'] >= 11547458]['Block Height']:\n",
    "    \n",
    "    print(f\"Snapshot for Block Height: {snapshot}\") \n",
    "\n",
    "    ## formating of data\n",
    "    # load data\n",
    "    ddf = pd.read_csv(\n",
    "        join(\n",
    "            path,\n",
    "            f\"token_balance_lookup_tables_labelled/df_token_balenace_labelled_greater_01pct_bh{snapshot}_v2.csv\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    ## rename column\n",
    "    ddf = ddf.rename(columns={'address_x': 'address'})\n",
    "\n",
    "    # filter data\n",
    "    ddf = ddf[ddf.value > 0]\n",
    "    ddf = ddf[ddf.token_address.isin(df_addresses.address) == True]\n",
    "\n",
    "    # remove known burner addresses\n",
    "    ddf = ddf[ddf.address.isin(known_burner_addresses) == False]\n",
    "    \n",
    "    ### fix total supply & check\n",
    "    dict_ts = dict(ddf.groupby('token_address').value.sum())\n",
    "    dict_ts.keys()\n",
    "\n",
    "    ## define supply \n",
    "    ddf['pct_supply'] = int\n",
    "\n",
    "    ## define supply\n",
    "    for t in dict_ts.keys(): \n",
    "\n",
    "        ddf.loc[ddf.token_address == t, 'pct_supply'] = ddf[ddf.token_address == t].value / dict_ts[t]\n",
    "    \n",
    "    # we set a cut of point of 0.001% \n",
    "    ddf = ddf[ddf.pct_supply > 0.000005]   \n",
    "        \n",
    "    # save coverage \n",
    "    pct_supply_coverage[str(snapshot)] = dict(ddf.groupby('token_address').pct_supply.sum())\n",
    "   \n",
    "    # population is the size of the possible token that can be held in the sample \n",
    "    pop_size = len(ddf.token_address.unique())\n",
    "    \n",
    "    # Define the batch size for computing the p-values\n",
    "    batch_size = 1024\n",
    "\n",
    "    # Generate all possible combinations\n",
    "    combinations_generator = combinations(ddf.address.unique(), 2) # generator expression to save memory\n",
    "\n",
    "    # calculate the number of combinations\n",
    "    num_combinations = math.comb(len(ddf.address.unique()), 2)\n",
    "\n",
    "\n",
    "    # Divide the combinations into batches    \n",
    "    batches = (list(islice(combinations_generator, batch_size)) for i in range(0, num_combinations, batch_size))\n",
    "\n",
    "\n",
    "    # Open the output file for writing\n",
    "    with open(join(output_path, f\"p_values_{snapshot}.pkl\"), \"ab\") as outfile:\n",
    "\n",
    "        # Create a ProcessPoolExecutor with the desired number of processes\n",
    "        with ProcessPoolExecutor(max_workers=16) as executor:\n",
    "            results = []\n",
    "            # Process each batch in parallel\n",
    "            for batch in tqdm(batches, total=num_combinations//batch_size, desc=\"Processing batches\"):\n",
    "                batch_result = executor.submit(process_batch, batch)\n",
    "                results.append(batch_result)\n",
    "\n",
    "            # Wait for all results to be returned\n",
    "            for future in tqdm(results, total=len(results), desc=\"Collecting results\"):\n",
    "                batch_result = future.result()\n",
    "                # Write the batch result to the pickle file\n",
    "                pickle.dump(batch_result, outfile)\n",
    "        \n",
    "            \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62638d3d-1bd4-4a19-a514-4daa9915d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the path to the pickle file\n",
    "pickle_file = join(output_path, f\"p_values_{snapshot}.pkl\")\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file, \"rb\") as infile:\n",
    "    batch_results = pickle.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e55dee1-ba2f-4098-93ec-0ac827c1285e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0x0bc529c00c6401aef6d220be8c6ea1667f6ad93e': 0.9906279410339102,\n",
       " '0x111111111117dc0aa78b770fa6a738034120c302': 0.9952467664631757,\n",
       " '0x1f9840a85d5af5bf1d1762f925bdaddc4201f984': 0.9820159211074848,\n",
       " '0x5a98fcbea516cf06857215779fd812ca3bef1b32': 0.9995789310018651,\n",
       " '0x6b3595068778dd592e39a122f4f5a5cf09c90fe2': 0.9915485719893664,\n",
       " '0x7fc66500c84a76ad7e9c93437bfc5ac33e2ddae9': 0.9855831631795628,\n",
       " '0x9f8f72aa9304c8b593d555f12ef6589cc3a579a2': 0.9872073780484134,\n",
       " '0xba100000625a3754423978a60c9317c58a424e3d': 0.9945502886777317,\n",
       " '0xc00e94cb662c3520282e6f5717214004a7f26888': 0.9951481912330175,\n",
       " '0xd533a949740bb3306d119cc777fa900ba034cd52': 0.996212692798453}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_supply_coverage['11659570']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feddfbdd-99f6-4f2b-b960-22a91bfe94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(batch_results, columns=['address1', 'address2', 'p-value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4ef6d-7917-41a9-9394-15b0820f72b2",
   "metadata": {},
   "source": [
    "### H5 file implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f21ee9f0-c9a8-4acd-bbda-889870ac538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot for Block Height: 11659570\n",
      "Snapshot for Block Height: 11861210\n",
      "Snapshot for Block Height: 12043054\n",
      "Snapshot for Block Height: 12244515\n",
      "Snapshot for Block Height: 12438842\n",
      "Snapshot for Block Height: 12638919\n",
      "Snapshot for Block Height: 12831436\n",
      "Snapshot for Block Height: 13029639\n",
      "Snapshot for Block Height: 13230157\n",
      "Snapshot for Block Height: 13422506\n",
      "Snapshot for Block Height: 13620205\n",
      "Snapshot for Block Height: 13809597\n",
      "Snapshot for Block Height: 14009885\n",
      "Snapshot for Block Height: 14210564\n",
      "Snapshot for Block Height: 14391029\n",
      "Snapshot for Block Height: 14589816\n",
      "Snapshot for Block Height: 14779829\n",
      "Snapshot for Block Height: 14967365\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import hypergeom \n",
    "from itertools import combinations, islice\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import os\n",
    "from os.path import join\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "path = os.environ[\"PROJECT_PATH\"]\n",
    "\n",
    "# Snapshot selection\n",
    "df_snapshot = pd.read_csv(\"../assets/snapshot_selection.csv\")\n",
    "\n",
    "# Address selection\n",
    "df_addresses = pd.read_csv(\"../assets/df_final_token_selection_20221209.csv\")\n",
    "\n",
    "# Burner addresses (remove burner addresses)\n",
    "known_burner_addresses = [\n",
    "    \"0x0000000000000000000000000000000000000000\",\n",
    "    \"0x0000000000000000000000000000000000000000\",\n",
    "    \"0x0000000000000000000000000000000000000001\",\n",
    "    \"0x0000000000000000000000000000000000000002\",\n",
    "    \"0x0000000000000000000000000000000000000003\",\n",
    "    \"0x0000000000000000000000000000000000000004\",\n",
    "    \"0x0000000000000000000000000000000000000005\",\n",
    "    \"0x0000000000000000000000000000000000000006\",\n",
    "    \"0x0000000000000000000000000000000000000007\",\n",
    "    \"0x000000000000000000000000000000000000dead\",\n",
    "]\n",
    "\n",
    "output_path = '/local/scratch/exported/governance-erc20/project_erc20_governance_data/wallet_projection_output/output_f-none'\n",
    "pct_supply_coverage_list = {}\n",
    "\n",
    "for snapshot in df_snapshot[df_snapshot['Block Height'] >= 11547458]['Block Height']:\n",
    "    print(f\"Snapshot for Block Height: {snapshot}\") \n",
    "\n",
    "    # Load data\n",
    "    ddf = pd.read_csv(join(path, f\"token_balance_lookup_tables_labelled/df_token_balenace_labelled_greater_01pct_bh{snapshot}_v2.csv\"))\n",
    "    ddf = ddf.rename(columns={'address_x': 'address'})\n",
    "    ddf = ddf[ddf.value > 0]\n",
    "    ddf = ddf[ddf.token_address.isin(df_addresses.address)]\n",
    "    ddf = ddf[~ddf.address.isin(known_burner_addresses)]\n",
    "    dict_ts = dict(ddf.groupby('token_address').value.sum())\n",
    "    ddf['pct_supply'] = 0\n",
    "    for t in dict_ts.keys(): \n",
    "        ddf.loc[ddf.token_address == t, 'pct_supply'] = ddf[ddf.token_address == t].value / dict_ts[t]\n",
    "    ddf = ddf[ddf.pct_supply > 0.000005] \n",
    "    # Save coverage\n",
    "    pct_supply_coverage = dict(ddf.groupby('token_address').pct_supply.sum())\n",
    "    \n",
    "    pct_supply_coverage_list[snapshot] = pct_supply_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f17f1367-f322-478d-9f2e-cb950b767761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11659570    0.982016\n",
       "11861210    0.978952\n",
       "12043054    0.976262\n",
       "12244515    0.976764\n",
       "12438842    0.975510\n",
       "12638919    0.977497\n",
       "12831436    0.976323\n",
       "13029639    0.975064\n",
       "13230157    0.974216\n",
       "13422506    0.973722\n",
       "13620205    0.972916\n",
       "13809597    0.973097\n",
       "14009885    0.972892\n",
       "14210564    0.972913\n",
       "14391029    0.972712\n",
       "14589816    0.973041\n",
       "14779829    0.972667\n",
       "14967365    0.972011\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pct_supply_coverage_list).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e0cde77-2436-47cb-accd-e33adbb5ac6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot for Block Height: 11659570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 4980it [00:00, 9849.66it/s]                           \n",
      "Collecting results: 100%|██████████| 4980/4980 [00:40<00:00, 123.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot for Block Height: 11861210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 5523it [00:00, 7937.74it/s]                           \n",
      "Collecting results: 100%|██████████| 5523/5523 [00:45<00:00, 121.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot for Block Height: 12043054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 5560it [00:00, 8199.85it/s]                           \n",
      "Collecting results: 100%|██████████| 5560/5560 [01:10<00:00, 78.79it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot for Block Height: 12244515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSnapshot for Block Height: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msnapshot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m ddf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_balance_lookup_tables_labelled/df_token_balenace_labelled_greater_01pct_bh\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msnapshot\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_v2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m ddf \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress_x\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     91\u001b[0m ddf \u001b[38;5;241m=\u001b[39m ddf[ddf\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/dtypes/common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[0;32m-> 1433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### .p5 file implementation\n",
    "\n",
    "# data  packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data manipulation packages\n",
    "import math\n",
    "from scipy.stats import hypergeom \n",
    "from itertools import combinations, islice\n",
    "\n",
    "# data processing \n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "\n",
    "# os \n",
    "import os\n",
    "from os.path import join\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# auxillary \n",
    "from tqdm import tqdm\n",
    "\n",
    "# data storage\n",
    "import h5py\n",
    "\n",
    "\n",
    "# load environment\n",
    "load_dotenv()\n",
    "\n",
    "path = os.environ[\"PROJECT_PATH\"]\n",
    "\n",
    "# Snapshot selection\n",
    "df_snapshot = pd.read_csv(\"../assets/snapshot_selection.csv\")\n",
    "\n",
    "# Address selection\n",
    "df_addresses = pd.read_csv(\"../assets/df_final_token_selection_20221209.csv\")\n",
    "\n",
    "# Burner addresses (remove burner addresses)\n",
    "known_burner_addresses = [\n",
    "    \"0x0000000000000000000000000000000000000000\",\n",
    "    \"0x0000000000000000000000000000000000000000\",\n",
    "    \"0x0000000000000000000000000000000000000001\",\n",
    "    \"0x0000000000000000000000000000000000000002\",\n",
    "    \"0x0000000000000000000000000000000000000003\",\n",
    "    \"0x0000000000000000000000000000000000000004\",\n",
    "    \"0x0000000000000000000000000000000000000005\",\n",
    "    \"0x0000000000000000000000000000000000000006\",\n",
    "    \"0x0000000000000000000000000000000000000007\",\n",
    "    \"0x000000000000000000000000000000000000dead\",\n",
    "]\n",
    "\n",
    "output_path = '/local/scratch/exported/governance-erc20/project_erc20_governance_data/wallet_projection_output/output_f-none'\n",
    "\n",
    "def main_wallets(wallet1, wallet2, pop_size, ddf):\n",
    "    # Unique tokens a given address holds\n",
    "    wallet1_uniqT = ddf[ddf.address == wallet1].token_address.unique()\n",
    "    wallet2_uniqT = ddf[ddf.address == wallet2].token_address.unique()\n",
    "\n",
    "    # Calculate intersection in token holdings (binary)\n",
    "    wallet1_wallet2_uniqT_intersection = np.intersect1d(wallet1_uniqT, wallet2_uniqT, assume_unique=True)\n",
    "\n",
    "    # Calculate hypergeometric\n",
    "    M = pop_size  # Number of tokens at a given snapshot    \n",
    "    n = len(wallet1_uniqT)  # Number of draws - Number of tokens in wallet1\n",
    "    K = len(wallet2_uniqT)  # Number of successes in population - Number of tokens in wallet2\n",
    "    x = len(wallet1_wallet2_uniqT_intersection)  # Number of successes in draws - Intersection of tokens in wallet1 and wallet2\n",
    "\n",
    "    # Compute the cumulative probability of obtaining at most x successes\n",
    "    pvalue = 1 - hypergeom.cdf(x, M, n, K)\n",
    "\n",
    "    return np.array((wallet1, wallet2, pvalue))\n",
    "\n",
    "# Define the function that will process a batch of combinations\n",
    "def process_batch(batch):\n",
    "    results = np.empty((len(batch), 3), dtype=object)\n",
    "    \n",
    "    for i, combination in enumerate(batch):\n",
    "        # Call your function here to process the combination\n",
    "        result = main_wallets(combination[0], combination[1], pop_size, ddf)\n",
    "        results[i] = result\n",
    "    return results\n",
    "\n",
    "\n",
    "pct_supply_coverage_list = {}\n",
    "\n",
    "for snapshot in df_snapshot[df_snapshot['Block Height'] >= 11547458]['Block Height']:\n",
    "    print(f\"Snapshot for Block Height: {snapshot}\") \n",
    "    \n",
    "    # Load data\n",
    "    ddf = pd.read_csv(join(path, f\"token_balance_lookup_tables_labelled/df_token_balenace_labelled_greater_01pct_bh{snapshot}_v2.csv\"))\n",
    "    ddf = ddf.rename(columns={'address_x': 'address'})\n",
    "    ddf = ddf[ddf.value > 0]\n",
    "    ddf = ddf[ddf.token_address.isin(df_addresses.address)]\n",
    "    ddf = ddf[~ddf.address.isin(known_burner_addresses)]\n",
    "    dict_ts = dict(ddf.groupby('token_address').value.sum())\n",
    "    ddf['pct_supply'] = 0\n",
    "    for t in dict_ts.keys(): \n",
    "        ddf.loc[ddf.token_address == t, 'pct_supply'] = ddf[ddf.token_address == t].value / dict_ts[t]\n",
    "    ddf = ddf[ddf.pct_supply > 0.000005] \n",
    "    \n",
    "    pop_size = len(ddf.token_address.unique())\n",
    "\n",
    "    # Save coverage\n",
    "    pct_supply_coverage = dict(ddf.groupby('token_address').pct_supply.sum())\n",
    "    \n",
    "     # Save in dict \n",
    "    pct_supply_coverage_list[snapshot] = pct_supply_coverage\n",
    "\n",
    "    # Create an HDF5 file for the current snapshot\n",
    "    file_name = f'output_snapshot_{snapshot}.h5'\n",
    "    \n",
    "    with h5py.File(join(output_path, file_name), 'w') as file:\n",
    "        \n",
    "        # Create a group for the current snapshot\n",
    "        snapshot_group = file.create_group(f'snapshot_{snapshot}')\n",
    "\n",
    "        # Define the batch size for computing the p-values\n",
    "        batch_size = 1024\n",
    "\n",
    "        # Generate all possible combinations\n",
    "        combinations_generator = combinations(ddf.address.unique(), 2)\n",
    "\n",
    "        # Calculate the number of combinations\n",
    "        num_combinations = math.comb(len(ddf.address.unique()), 2)\n",
    "\n",
    "        # Divide the combinations into batches\n",
    "        batches = (list(islice(combinations_generator, batch_size)) for i in range(0, num_combinations, batch_size))\n",
    "\n",
    "        # Create a group for storing the p-values\n",
    "        p_values_group = snapshot_group.create_group('p_values')\n",
    "\n",
    "        # Open the output file for writing\n",
    "        with ProcessPoolExecutor(max_workers=12) as executor:\n",
    "            results = []\n",
    "            # Process each batch in parallel\n",
    "            for batch in batches:\n",
    "                batch_result = executor.submit(process_batch, batch)\n",
    "                results.append(batch_result)\n",
    "\n",
    "            # Wait for all results to be returned\n",
    "            for i, future in enumerate(results):\n",
    "                batch_result = future.result()\n",
    "                # Write the batch result to the HDF5 file\n",
    "                p_values_group.create_dataset(f'batch_{i}', data=batch_result) \n",
    "                \n",
    "\n",
    "df = pd.DataFrame(pct_supply_coverage_list)\n",
    "df.to_csv(join(output_path, \"pct_supply_coverage.csv\"))\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
