{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "path = os.environ['PROJECT_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of unique sender & receiver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETAINED METHODOLOGICAL DOCUMENTATION\n",
    "\n",
    "# load unique governance tokens transfers \n",
    "df_tt = pd.read_csv(join(path, 'token_transfers_22021209_refined.csv'))\n",
    "\n",
    "# unique sender\n",
    "unique_sender = df_tt.from_address.unique()\n",
    "\n",
    "# unique receivr \n",
    "unique_receiver = df_tt.to_address.unique()\n",
    "\n",
    "# sender & receiver  \n",
    "array_SR= np.concatenate([unique_receiver, unique_sender])\n",
    "\n",
    "# remove dublicates between unique sender & receiver \n",
    "unique_addresses = np.unique(array_SR)\n",
    "\n",
    "# convert to csv for storage \n",
    "df_unique_addresses = pd. DataFrame(unique_addresses, columns=['unique_addresses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bash Manipulation\n",
    "Step1: consolidate all csv files using ubuntu comand line: \n",
    "> nohup awk '(NR == 1) || (FNR > 1)' transactions_merged_to_11307940.csv transactions_to_15050010.csv > transactions_merged_all.csv\n",
    "\n",
    "\n",
    "Step2: Filter out unique_addresses out of transactions_merged_all and store in new df\n",
    "> awk -F, '(NR==FNR){a[$2];next}(($6 in a) || ($7 in a))' df_unique_addresses2.csv transactions_merged_all.csv > tx_all_uniq_addresses2.csv\n",
    "\n",
    "Step3: Remove columns \n",
    "> cut -d, -f2,3,5,11,13-15 --complement tx_all_uniq_addresses_selected.csv \n",
    "\n",
    "Rational: \n",
    "- Irrelevant for culstering heuristic\n",
    "\n",
    "Ref.: https://linuxconfig.org/how-to-remove-columns-from-csv-based-on-column-number-using-bash-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding on relevant columns in Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "       'hash', # unique identifier of tx\n",
    "       'nonce', # --> remove\n",
    "       'block_hash', # --> remove\n",
    "       'block_number', # necessary when filtering for snapshot dates\n",
    "       'transaction_index', # --> remove\n",
    "       'from_address', # --> network node \n",
    "       'to_address', # --> network node\n",
    "       'value', # --> weight \n",
    "       'gas', # --> clustering heuristic gas pattern (Beres et al. 2020)\n",
    "       'gas_price', # --> clustering heuristic gas pattern (Beres et al. 2020)\n",
    "       'input', # --> indicates smart contract interaction\n",
    "       'block_timestamp', # --> remove can be inferred from blockheight\n",
    "       'max_fee_per_gas', # -->\n",
    "       'max_priority_fee_per_gas', # -->\n",
    "       'transaction_type'# -->\n",
    "       ]\n",
    "\n",
    "selected_columns = ['hash', 'block_number','from_address', 'to_address', 'value', 'gas', 'gas_price', 'input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(join(path,'tx_all_uniq_addresses_reduced.csv'), dtype='str', names=['hash', 'block_number','from_address', 'to_address', 'value', 'gas', 'gas_price','block_timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other & Out-Takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### removing tokens \n",
    "import web3 as Web3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Web3.Web3.toChecksumAddress('0x3432b6a60d23ca0dfca7761b7ab56459d9c964d0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grep command to delete Gnosis( ) and Frax-Share ( ), as Gnosis is fragmenting in terms of Governance power and Frax is a non-standard ERC20\n",
    "\n",
    "> grep -v -e '0x6810e776880C02933D47DB1b9fc05908e5386b96' -e '0x3432b6a60d23ca0dfca7761b7ab56459d9c964d0' token_transfers_22021209.csv > token_transfers_22021209_refined.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
