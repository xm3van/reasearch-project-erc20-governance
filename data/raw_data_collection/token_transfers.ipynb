{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/eisermann/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/user/eisermann/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:59: UserWarning: Pandas requires version '1.3.2' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'PROJECT_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      8\u001b[0m load_dotenv()  \n\u001b[0;32m---> 10\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPROJECT_PATH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.9/os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'PROJECT_PATH'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  \n",
    "\n",
    "path = os.environ['PROJECT_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of unique sender & receiver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETAINED METHODOLOGICAL DOCUMENTATION\n",
    "\n",
    "# load unique governance tokens transfers \n",
    "df_tt = pd.read_csv(join(path, 'token_transfers_22021209_refined.csv'))\n",
    "\n",
    "# unique sender\n",
    "unique_sender = df_tt.from_address.unique()\n",
    "\n",
    "# unique receivr \n",
    "unique_receiver = df_tt.to_address.unique()\n",
    "\n",
    "# sender & receiver  \n",
    "array_SR= np.concatenate([unique_receiver, unique_sender])\n",
    "\n",
    "# remove dublicates between unique sender & receiver \n",
    "unique_addresses = np.unique(array_SR)\n",
    "\n",
    "# convert to csv for storage \n",
    "df_unique_addresses = pd. DataFrame(unique_addresses, columns=['unique_addresses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bash Manipulation\n",
    "Step1: consolidate all csv files using ubuntu comand line: \n",
    "> nohup awk '(NR == 1) || (FNR > 1)' transactions_merged_to_11307940.csv transactions_to_15050010.csv > transactions_merged_all.csv\n",
    "\n",
    "\n",
    "Step2: Filter out unique_addresses out of transactions_merged_all and store in new df\n",
    "> awk -F, '(NR==FNR){a[$2];next}(($6 in a) || ($7 in a))' df_unique_addresses2.csv transactions_merged_all.csv > tx_all_uniq_addresses2.csv\n",
    "\n",
    "Step3: Remove columns \n",
    "> cut -d, -f2,3,5,11,13-15 --complement tx_all_uniq_addresses_selected.csv \n",
    "\n",
    "Rational: \n",
    "- Irrelevant for culstering heuristic\n",
    "\n",
    "Ref.: https://linuxconfig.org/how-to-remove-columns-from-csv-based-on-column-number-using-bash-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding on relevant columns in Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "       'hash', # unique identifier of tx\n",
    "       'nonce', # --> remove\n",
    "       'block_hash', # --> remove\n",
    "       'block_number', # necessary when filtering for snapshot dates\n",
    "       'transaction_index', # --> remove\n",
    "       'from_address', # --> network node \n",
    "       'to_address', # --> network node\n",
    "       'value', # --> weight \n",
    "       'gas', # --> clustering heuristic gas pattern (Beres et al. 2020)\n",
    "       'gas_price', # --> clustering heuristic gas pattern (Beres et al. 2020)\n",
    "       'input', # --> indicates smart contract interaction\n",
    "       'block_timestamp', # --> remove can be inferred from blockheight\n",
    "       'max_fee_per_gas', # -->\n",
    "       'max_priority_fee_per_gas', # -->\n",
    "       'transaction_type'# -->\n",
    "       ]\n",
    "\n",
    "selected_columns = ['hash', 'block_number','from_address', 'to_address', 'value', 'gas', 'gas_price', 'input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(join(path,'tx_all_uniq_addresses_reduced.csv'), dtype='str', names=['hash', 'block_number','from_address', 'to_address', 'value', 'gas', 'gas_price','block_timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other & Out-Takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### removing tokens \n",
    "import web3 as Web3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Web3.Web3.toChecksumAddress('0x3432b6a60d23ca0dfca7761b7ab56459d9c964d0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grep command to delete Gnosis( ) and Frax-Share ( ), as Gnosis is fragmenting in terms of Governance power and Frax is a non-standard ERC20\n",
    "\n",
    "> grep -v -e '0x6810e776880C02933D47DB1b9fc05908e5386b96' -e '0x3432b6a60d23ca0dfca7761b7ab56459d9c964d0' token_transfers_22021209.csv > token_transfers_22021209_refined.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
